name: Performance Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      test_size:
        description: 'Test size (small, medium, large)'
        required: false
        default: 'medium'
        type: choice
        options:
        - small
        - medium
        - large

env:
  PYTHONPATH: ${{ github.workspace }}/src

jobs:
  performance-test:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: collatz_db_perf
          POSTGRES_USER: postgresql
          POSTGRES_PASSWORD: perf_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark memory-profiler psutil
    
    - name: Set up performance database
      env:
        PGPASSWORD: perf_password
      run: |
        psql -h localhost -U postgresql -d collatz_db_perf -f sql/01_create_tables.sql
        psql -h localhost -U postgresql -d collatz_db_perf -f sql/02_create_indexes.sql
        psql -h localhost -U postgresql -d collatz_db_perf -f sql/03_create_views.sql
        psql -h localhost -U postgresql -d collatz_db_perf -f sql/04_create_functions.sql
    
    - name: Discover test files
      run: |
        echo "Discovering test files in tests/ directory:"
        ls tests/test_*.py | sort
        echo "Total test files found: $(ls tests/test_*.py | wc -l)"
    
    - name: Run performance tests
      env:
        DB_HOST: localhost
        DB_PORT: 5432
        DB_NAME: collatz_db_perf
        DB_USER: postgresql
        DB_PASSWORD: perf_password
        TEST_SIZE: ${{ github.event.inputs.test_size || 'medium' }}
      run: |
        # Check if there are actual test scripts in the tests directory
        TEST_SCRIPTS=$(find tests/ -name "test_*.py" -o -name "*_test.py" | wc -l)
        if [ "$TEST_SCRIPTS" -eq 0 ]; then
          echo "No test scripts found in tests/ directory. Skipping performance tests."
          exit 0
        fi
        
        # Run performance tests with benchmarking on all test files
        pytest tests/ -v \
          --benchmark-only \
          --benchmark-json=benchmark_results.json \
          --benchmark-sort=mean \
          --benchmark-columns=min,max,mean,stddev,rounds,iterations
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: |
          benchmark_results.json
          .benchmarks/
        retention-days: 30
    
    - name: Performance regression check
      run: |
        # Simple performance regression check
        python << 'EOF'
        import json
        import sys
        
        try:
            with open('benchmark_results.json', 'r') as f:
                results = json.load(f)
            
            # Check if any benchmark is significantly slower than expected
            for benchmark in results['benchmarks']:
                mean_time = benchmark['stats']['mean']
                name = benchmark['name']
                
                # Set performance thresholds based on test type
                if 'small_numbers' in name and mean_time > 0.001:  # 1ms
                    print(f"WARNING: {name} is slower than expected: {mean_time:.6f}s")
                elif 'medium_numbers' in name and mean_time > 0.01:  # 10ms
                    print(f"WARNING: {name} is slower than expected: {mean_time:.6f}s")
                elif 'large_numbers' in name and mean_time > 0.1:  # 100ms
                    print(f"WARNING: {name} is slower than expected: {mean_time:.6f}s")
                elif 'database' in name and mean_time > 0.05:  # 50ms
                    print(f"WARNING: {name} is slower than expected: {mean_time:.6f}s")
                else:
                    print(f"✓ {name}: {mean_time:.6f}s (within expected range)")
        
        except FileNotFoundError:
            print("No benchmark results found")
            sys.exit(1)
        EOF
    
    - name: Create performance summary
      if: always()
      run: |
        echo "## Performance Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f benchmark_results.json ]; then
          echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Performance tests completed successfully. Key metrics:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          python << 'EOF' >> $GITHUB_STEP_SUMMARY
        import json
        
        try:
            with open('benchmark_results.json', 'r') as f:
                results = json.load(f)
            
            print("| Test Name | Mean Time | Min Time | Max Time |")
            print("|-----------|-----------|----------|----------|")
            
            for benchmark in results['benchmarks']:
                name = benchmark['name'].replace('test_', '').replace('_', ' ').title()
                mean = f"{benchmark['stats']['mean']:.6f}s"
                min_time = f"{benchmark['stats']['min']:.6f}s"
                max_time = f"{benchmark['stats']['max']:.6f}s"
                print(f"| {name} | {mean} | {min_time} | {max_time} |")
        
        except:
            print("Could not parse benchmark results")
        EOF
        else
          echo "❌ Performance tests failed or no results generated" >> $GITHUB_STEP_SUMMARY
        fi

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install memory-profiler matplotlib
    
    - name: Run memory profiling on test files
      run: |
        # Run memory profiling on existing test files that have memory-related tests
         if [ -f "tests/test_performance.py" ]; then
           echo "Running memory profiling on performance tests..."
           python -m memory_profiler tests/test_performance.py > memory_profile_output.txt 2>&1 || echo "Memory profiling completed with warnings"
         else
           echo "No performance test file found for memory profiling"
           echo "Available test files:" > memory_profile_output.txt
           ls tests/test_*.py >> memory_profile_output.txt 2>/dev/null || echo "No test files found" >> memory_profile_output.txt
         fi
    
    - name: Upload memory profile results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: memory-profile-results
        path: memory_profile_output.txt
        retention-days: 30